{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01-TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will here compute the TF-IDF on a corpus of newspaper headlines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by importing needed libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import needed libraries\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the data into the file *headlines.csv*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20170721</td>\n",
       "      <td>algorithms can make decisions on behalf of fed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20170721</td>\n",
       "      <td>andrew forrests fmg to appeal pilbara native t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20170721</td>\n",
       "      <td>a rural mural in thallan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20170721</td>\n",
       "      <td>australia church risks becoming haven for abusers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20170721</td>\n",
       "      <td>australian company usgfx embroiled in shanghai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publish_date                                      headline_text\n",
       "0      20170721  algorithms can make decisions on behalf of fed...\n",
       "1      20170721  andrew forrests fmg to appeal pilbara native t...\n",
       "2      20170721                           a rural mural in thallan\n",
       "3      20170721  australia church risks becoming haven for abusers\n",
       "4      20170721  australian company usgfx embroiled in shanghai..."
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Load the dataset\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"headlines.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, check the dataset basic information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   publish_date                                      headline_text\n",
      "0      20170721  algorithms can make decisions on behalf of fed...\n",
      "1      20170721  andrew forrests fmg to appeal pilbara native t...\n",
      "2      20170721                           a rural mural in thallan\n",
      "3      20170721  australia church risks becoming haven for abusers\n",
      "4      20170721  australian company usgfx embroiled in shanghai...\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1999 entries, 0 to 1998\n",
      "Data columns (total 2 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   publish_date   1999 non-null   int64 \n",
      " 1   headline_text  1999 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 31.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# TODO: Have a look at the data\n",
    "print(df.head())\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now perform preprocessing on this text data: tokenization, punctuation and stop words removal and stemming.\n",
    "\n",
    "Hint: to do so, use NLTK, *pandas*'s method *apply*, lambda functions and list comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                algorithm make decis behalf feder minist\n",
      "1       andrew forrest fmg appeal pilbara nativ titl rule\n",
      "2                                     rural mural thallan\n",
      "3                        australia church risk becom abus\n",
      "4       australian compani usgfx embroil shanghai staf...\n",
      "                              ...                        \n",
      "1994    constitut avenu win top prize act architectu a...\n",
      "1995                              dark mofo number crunch\n",
      "1996    david petraeu say australia must firm south ch...\n",
      "1997    driverless car australia face challeng roo pro...\n",
      "1998                     drug compani criticis price hike\n",
      "Name: stemmed, Length: 1999, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# TODO: Perform preprocessing\n",
    "# import needed modules\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "\n",
    "def preprocess(text):\n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove punctuation\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Stem\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df['stemmed'] = df['headline_text'].apply(preprocess)\n",
    "# Show the output\n",
    "print(df['stemmed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute now the Bag of Words for our data, using scikit-learn.\n",
    "\n",
    "Warning: since we used our own preprocessing, you have to bypass analyzer with identity function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1999, 38)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Compute the BOW of the preprocessed data\n",
    "# Import the library\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "# Compute the BOW representation of the preprocessed data\n",
    "vectorizer = CountVectorizer(analyzer=lambda x: x, stop_words='english')\n",
    "bow = vectorizer.fit_transform(df['stemmed']).toarray()\n",
    "print(bow.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the shape of the BOW, the expected value is `(1999, 4165)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute the Term Frequency and then the Inverse Document Frequency, and check the values are not only zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.36008246e-01 7.54204691e-05 1.19415743e-03 1.03074641e-03\n",
      " 8.67335395e-04 3.14251955e-04 3.01681876e-04 2.63971642e-04\n",
      " 2.51401564e-04 4.52522815e-04 2.38831486e-04 3.01681876e-04\n",
      " 8.51497096e-02 1.89933881e-02 3.86655605e-02 3.11486537e-02\n",
      " 8.10644342e-02 1.51972245e-02 1.87545567e-02 2.38957186e-02\n",
      " 6.70487970e-02 3.10480931e-03 1.22809664e-02 4.73640546e-02\n",
      " 2.75913216e-02 5.86394147e-02 5.60876889e-02 2.46876336e-02\n",
      " 1.84780149e-03 7.34846771e-02 4.73389144e-02 5.85639943e-02\n",
      " 3.12366443e-02 9.45269880e-03 1.42418986e-02 2.42602509e-03\n",
      " 9.12587676e-03 1.30728813e-03]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Compute the TF using the BOW\n",
    "\n",
    "tf = np.sum(bow, axis=0) / np.sum(bow)\n",
    "print(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 31)\t0.1729787380501601\n",
      "  (0, 30)\t0.18357889090424717\n",
      "  (0, 29)\t0.16659403809380424\n",
      "  (0, 26)\t0.08749484572182369\n",
      "  (0, 25)\t0.08609938784881604\n",
      "  (0, 24)\t0.32930566581031706\n",
      "  (0, 23)\t0.1837648442057908\n",
      "  (0, 22)\t0.15309537938449247\n",
      "  (0, 20)\t0.34095844679515935\n",
      "  (0, 19)\t0.2348496708070992\n",
      "  (0, 18)\t0.12972178878245524\n",
      "  (0, 17)\t0.2902351341817359\n",
      "  (0, 16)\t0.40884261448336146\n",
      "  (0, 15)\t0.2147088649368237\n",
      "  (0, 14)\t0.10011226400742812\n",
      "  (0, 13)\t0.1298723097496903\n",
      "  (0, 12)\t0.24444796124505672\n",
      "  (0, 0)\t0.39438269757200334\n",
      "  (1, 34)\t0.11788948961467856\n",
      "  (1, 33)\t0.13754041088197883\n",
      "  (1, 32)\t0.08465191760659271\n",
      "  (1, 31)\t0.2812233113573361\n",
      "  (1, 30)\t0.07461417538557386\n",
      "  (1, 29)\t0.3385540874360546\n",
      "  (1, 27)\t0.28293790711235167\n",
      "  :\t:\n",
      "  (1997, 18)\t0.1053944202990478\n",
      "  (1997, 17)\t0.1179029521740513\n",
      "  (1997, 16)\t0.3321703373924552\n",
      "  (1997, 15)\t0.08722172491404796\n",
      "  (1997, 14)\t0.24401314834432428\n",
      "  (1997, 13)\t0.10551671332501891\n",
      "  (1997, 12)\t0.3972108527105254\n",
      "  (1997, 0)\t0.38450659223901523\n",
      "  (1998, 32)\t0.11712013799367085\n",
      "  (1998, 31)\t0.09727160932808901\n",
      "  (1998, 30)\t0.10323242242490897\n",
      "  (1998, 29)\t0.2810438503456461\n",
      "  (1998, 27)\t0.2609724438741541\n",
      "  (1998, 26)\t0.09840243427844543\n",
      "  (1998, 25)\t0.09683301095408699\n",
      "  (1998, 24)\t0.12345290693063399\n",
      "  (1998, 22)\t0.1721810911709347\n",
      "  (1998, 20)\t0.5751963022904028\n",
      "  (1998, 19)\t0.1320636610434407\n",
      "  (1998, 18)\t0.14589362024515226\n",
      "  (1998, 16)\t0.1839244731276416\n",
      "  (1998, 15)\t0.12073782630646734\n",
      "  (1998, 14)\t0.4503712372167831\n",
      "  (1998, 12)\t0.09164072939562447\n",
      "  (1998, 0)\t0.3548388905263727\n"
     ]
    }
   ],
   "source": [
    "# TODO: Compute the IDF\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "idf = TfidfTransformer(use_idf=True).fit_transform(bow)\n",
    "\n",
    "print(idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute finally the TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: compute the TF-IDF\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer()\n",
    "tfidf = transformer.fit_transform(bow)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the 10 words with the highest and lowest TF-IDF on average?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 words with the lowest average TF-IDF:\n",
      "['.', '8', '5', '6', '9', '4', '3', '7', '2', '1']\n",
      "\n",
      "10 words with the highest average TF-IDF:\n",
      "[' ', 'a', 'e', 'r', 'i', 't', 'n', 'o', 's', 'l']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Print the 10 words with the highest and lowest TF-IDF on average\n",
    "\n",
    "avg_tfidf = np.asarray(tfidf.mean(axis=0)).flatten()\n",
    "ind_sorted = np.argsort(avg_tfidf)\n",
    "\n",
    "lowest_tfidf = [vectorizer.get_feature_names()[ind] for ind in ind_sorted[:10]]\n",
    "highest_tfidf = [vectorizer.get_feature_names()[ind] for ind in ind_sorted[-10:][::-1]]\n",
    "\n",
    "print(\"10 words with the lowest average TF-IDF:\")\n",
    "print(lowest_tfidf)\n",
    "print(\"\\n10 words with the highest average TF-IDF:\")\n",
    "print(highest_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the TF-IDF using scikit-learn on our preprocessed data (the one you used to compute the BOW)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute the TF-IDF using scikit learn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "tfidf = tfidf_vectorizer.fit_transform(df['stemmed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the 10 highest and lowest TF-IDF words on average to the ones you had by yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 words with the highest average TF-IDF:\n",
      "['australia', 'australian', 'new', 'polic', 'say', 'trump', 'man', 'wa', 'charg', 'sydney']\n",
      "\n",
      "10 words with the lowest average TF-IDF:\n",
      "['adel', 'melb', 'haw', 'coll', 'gw', 'syd', 'gcfc', 'nmfc', 'geel', 'fabio']\n"
     ]
    }
   ],
   "source": [
    "# TODO: Print the 10 words with the highest and lowest TF-IDF on average\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "avg_tfidf = tfidf.mean(axis=0).A1\n",
    "\n",
    "sorted_tfidf = np.argsort(avg_tfidf)\n",
    "\n",
    "print(\"10 words with the highest average TF-IDF:\")\n",
    "print([feature_names[i] for i in sorted_tfidf[-10:][::-1]])\n",
    "print(\"\\n10 words with the lowest average TF-IDF:\")\n",
    "print([feature_names[i] for i in sorted_tfidf[:10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you have the same words? How do you explain it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are different. The implementation of the two methods may differ, which could lead to small numerical differences that affect the order of words in the final TF-IDF matrix. For example, the way term frequencies and inverse document frequencies are computed in the two implementations may be slightly different, which could affect the final TF-IDF scores and order of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
